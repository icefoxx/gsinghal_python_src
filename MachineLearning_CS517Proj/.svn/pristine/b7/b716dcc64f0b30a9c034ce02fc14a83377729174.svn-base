#!/usr/bin/env python
import re, math

import os
import csv
import shutil
from os import environ
from os.path import dirname
from os.path import join
from os.path import exists
from os.path import expanduser
from os.path import isdir
from os import listdir
from os import makedirs

import numpy as np

class Bunch(dict):
    """Container object for datasets: dictionary-like object that
       exposes its keys as attributes."""

    def __init__(self, **kwargs):
        dict.__init__(self, kwargs)
        self.__dict__ = self


def convert_file2dictionary(filename,dictKeyField):
	fTasks=open(filename,"r")
	original_lines=fTasks.readlines()
	fTasks.close()
	original_fields=map(lambda s:s.strip('"'),original_lines[0].strip("\n").strip("\r").split("\t"))
	all_items=dict()
	numrows = 0
	for t in original_lines[1:]:
		numrows += 1
		#print "processing row %s" % numrows
		item_dictionary={}
		for k,v in map(None,original_fields,t.split('\t')):
			item_dictionary[k]=v.strip("\n").strip("\r").strip('"')
		all_items[item_dictionary["TargetID"]]=item_dictionary
	print "processed %s rows" % numrows
	return all_items

def convert_file2dictionary2(filename,dictKeyField):
	fTasks=open(filename,"r")
	original_lines=fTasks.readlines()
	fTasks.close()
	original_fields=map(lambda s:s.strip('"'),original_lines[0].strip("\n").strip("\r").split("\t"))
	all_items=dict()
	for name in original_fields[1:]:
		all_items[name] = dict()
	numrows = 0
	for t in original_lines[1:]:
		numrows += 1
		#print "processing row %s" % numrows
		item_dictionary={}
		for k,v in map(None,original_fields,t.split('\t')):
			item_dictionary[k]=v.strip("\n").strip("\r").strip('"')
		for k,v in all_items.iteritems():
		  all_items[k][item_dictionary["TargetID"]] = item_dictionary[k]
	print "processed %s rows" % numrows
	return all_items


def run():
	result = convert_file2dictionary("train.tsv","TargetID") #row major
	result2 = convert_file2dictionary2("train.tsv", "TargetID") #col major
	print "dictionary has %s items" % len(result2)

	# reprint the data into a new file, column major
	# this now prints the data out to a .csv file that is suitable for sklearn
	with open("skdata.csv", "w") as f:
		cols = sorted(result.keys())  # this gives us gene names in order
		rows = sorted(result2.keys())  # this gives us patient names in order

		firstline = "%s,%s,%s,%s\n"%(len(rows), len(cols), "AD", "CON")
		f.write(firstline)
		for i in xrange(len(rows)):
			line = ""
			row = result2["%s"%rows[i]]
			for j in xrange(len(cols)):
				line = "%s,%s" %(line,row["%s"%cols[j]].replace("NaN","0"))
			line = line.lstrip(",") # we need to strip the leading comma
			label = ""
			if re.search("AD", rows[i]):
				label = "0" # kind of a bad way to assign labels, but whatever
			else:
				label = "1"
			line = "%s,%s\n" %(line,label)
			f.write(line)
	f.close()

	dataset = load_data(fnames=cols)
	print "data: %s\n" % dataset.data
	print "targets: %s\n" % dataset.target
	print "target names: %s\n" % dataset.target_names

##	# computes mean and std of the values of AD vs CON for each GI
##	with open("stats.txt", "w") as f:
##		f.write("TargetID\tmeanAD\tsdAD\tmeanCON\tsdCON\tdiff\n")
##		stats = []
##		for k,row in result.iteritems():
##			# each row corresponds to data from one GI
##			valuesAD = []
##			valuesCON = []
##			for (k,v) in row.iteritems():
##				if re.search("AD", k):
##					valuesAD.append(v)
##				elif re.search("CON", k):
##					valuesCON.append(v)
##			row_dict = dict()
##			row_dict["meanAD"] = getMean(valuesAD)
##			row_dict["sdAD"] = getSD(valuesAD)
##			row_dict["meanCON"] = getMean(valuesCON)
##			row_dict["sdCON"] = getSD(valuesCON)
##			row_dict["TargetID"] = row["TargetID"]
##			row_dict["diff"] = diff(valuesAD, valuesCON)
##			stats.append(row_dict)
##			f.write("%s\t%s\t%s\t%s\t%s\t%s\n" % (row_dict["TargetID"],row_dict["meanAD"],row_dict["sdAD"],row_dict["meanCON"],row_dict["sdCON"],row_dict["diff"]))
##		print "computed stats for %s GIs" % len(stats)
##	f.close()



##	top = shrink(stats, size=20)
##	top_names = map(lambda x: x["TargetID"], top)
##	print top_names
##
##	lean_results = []
##	for k,row in result.iteritems():
##		if row["TargetID"] in top_names:
##			lean_results.append(row)
##	print "lean results contain %s results" % (len(lean_results))
##	# printing lean results into file
##	with open("output_lean.txt", "w") as f:
##		cols = sorted(lean_results[0].keys())  # this gives us column names in order
##		firstrow_string = ""
##		for col_name in cols:
##			firstrow_string = "%s\t%s" %(firstrow_string, col_name)  # concat column names to first line, tab separated
##		f.write("%s\n" % (firstrow_string.lstrip("\t")))  # stripping leading tab
##		for row in lean_results:
##			row_string = ""
##			for col_name in cols:
##				row_string = "%s\t%s" %(row_string, row[col_name].replace("NaN", "NaN"))
##			f.write("%s\n" %(row_string.lstrip("\t")))
##	f.close()
def load_data(fnames=[]):
	data_file = csv.reader(open("skdata.csv"))
	temp = data_file.next()
	n_samples = int(temp[0])
	n_features = int(temp[1])
	target_names = np.array(temp[2:])
	data = np.empty((n_samples, n_features))
	target = np.empty((n_samples,), dtype=np.int)

	for i, ir in enumerate(data_file):
		data[i] = np.asarray(ir[:-1], dtype=np.float)
		target[i] = np.asarray(ir[-1], dtype=np.int)

	return Bunch(data=data, target=target,
				 target_names=target_names,
				 DESCR="big dataset",
				 feature_names=fnames)

def diff(v1, v2):
	mean1 = getMean(v1)
	sd1 = getSD(v1)
	mean2 = getMean(v2)
	sd2 = getSD(v2)
	return math.fabs(mean1-mean2)/((sd1+sd2)/2)

def getMean(v):
	floatv = toFloat(v)
	return (sum(floatv)/len(floatv))

def getSD(v):
	floatv = toFloat(v)
	mean = getMean(floatv)
	sd = math.sqrt(sum((x-mean)**2 for x in floatv)/len(floatv))
	return sd

def toFloat(v):
	temp = []
	for each in v:
		if not each == "NaN":
			temp.append(float(each))
	return temp

def shrink(d, size=3, key="diff"):
	s = sorted(d, key=lambda x: x[key])
	return s[-size:]


def main():
	run()

if __name__ == '__main__':
	main()
